import torch
from torch.utils.data import Dataset
from torch.nn.utils.rnn import pad_sequence
import numpy as np
import os
import pickle
from scipy.signal import butter, filtfilt

# ==========================================
# Data Augmentation (Jittering & Scaling)
# ==========================================
class DataAugmenter:
    """
    Apply augmentation techniques to time-series trajectory data.
    Techniques: Jittering (Noise), Scaling
    """
    def __init__(self, jitter_sigma=0.03, scale_sigma=0.05):
        self.jitter_sigma = jitter_sigma
        self.scale_sigma = scale_sigma

    def jitter(self, x):
        # Add random noise
        noise = np.random.normal(loc=0, scale=self.jitter_sigma, size=x.shape)
        return x + noise

    def scale(self, x):
        # Scale the trajectory size
        factor = np.random.normal(loc=1.0, scale=self.scale_sigma, size=(1, x.shape[1])) 
        return x * factor

    def augment(self, x):
        # Online Augmentation: Apply random changes
        if np.random.rand() < 0.5:
            x = self.jitter(x)
        if np.random.rand() < 0.5:
            x = self.scale(x)
        return x

# ==========================================
# Filtering Logic
# ==========================================
def butter_lowpass_filter(data, cutoff=3.0, fs=30.0, order=5):
    """
    Applies a low-pass Butterworth filter to smooth the sensor data.
    """
    nyquist = 0.5 * fs
    normal_cutoff = cutoff / nyquist
    try:
        b, a = butter(order, normal_cutoff, btype="low", analog=False)
        # axis=0: Time dimension filtering
        return filtfilt(b, a, data, axis=0)
    except Exception as e:
        print(f"Filter Error: {e}")
        return data

# ==========================================
# Dataset & Loader
# ==========================================
class SensorDataset(Dataset):
    def __init__(self, sequences, labels, augment=False):
        self.sequences = sequences
        self.labels = labels
        self.augment = augment
        self.augmenter = DataAugmenter()
        
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        seq = self.sequences[idx]
        label = self.labels[idx]
        
        # Apply augmentation only for training set if requested
        if self.augment:
            seq_np = seq.numpy()
            seq_np = self.augmenter.augment(seq_np)
            seq = torch.tensor(seq_np, dtype=torch.float32)
            
        return seq, label

def load_data(pkl_path, apply_filter=True):
    """
    Loads dataset from a .pkl file generated by the data loader.
    Expected dict keys: 'sequences', 'labels', 'label_map'
    """
    if not os.path.exists(pkl_path):
        raise FileNotFoundError(f"Pickle file not found: {pkl_path}")

    print(f"Loading data from {pkl_path}...")
    
    try:
        with open(pkl_path, 'rb') as f:
            data = pickle.load(f)
            
        # Extract data from the dictionary
        raw_sequences = data['sequences']
        raw_labels = data['labels']
        label_map = data['label_map']
        
        sequences = []
        labels = []
        
        # Process sequences (Filter and Convert to Tensor)
        for seq, label in zip(raw_sequences, raw_labels):
            # Ensure it is a numpy array of float32
            seq_np = np.array(seq, dtype=np.float32)
            
            # Apply Low-pass filter if requested (recommended for raw data)
            if apply_filter:
                seq_np = butter_lowpass_filter(seq_np)
                
            sequences.append(torch.tensor(seq_np.copy(), dtype=torch.float32))
            labels.append(label)
            
        print(f"Successfully loaded {len(sequences)} sequences.")
        print(f"Classes: {label_map}")
        
        return sequences, labels, label_map
        
    except Exception as e:
        print(f"Error loading pickle file: {e}")
        return [], [], {}

def collate_fn(batch):
    sequences, labels = zip(*batch)
    
    # Sort by length for pack_padded_sequence
    lengths = torch.tensor([len(seq) for seq in sequences])
    sorted_lengths, sorted_idx = lengths.sort(descending=True)
    
    sorted_sequences = [sequences[i] for i in sorted_idx]
    sorted_labels = torch.tensor([labels[i] for i in sorted_idx], dtype=torch.long)
    
    # Padding (Variable length handling)
    padded_seqs = pad_sequence(sorted_sequences, batch_first=True)
    
    return padded_seqs, sorted_labels, sorted_lengths